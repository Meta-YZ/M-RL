{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**文件构成：**\n",
    "- Replayer Buffer\n",
    "- 策略网络和价值函数网络构建\n",
    "- Agent\n",
    "- 各种初始化方法\n",
    "- main方法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import torch\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "from gym.spaces import Box\n",
    "from config import get_config\n",
    "from tensorboardX import SummaryWriter\n",
    "from collections import namedtuple, deque\n",
    "from torch.nn.utils import clip_grad_norm_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ReplayBuffer的构建\n",
    "\"\"\"\n",
    "class ReplayBuffer:\n",
    "    \"\"\"存储轨迹转移数组\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        self.action_size = action_size\n",
    "        self.buffer = deque(maxlen=buffer_size)  # 一个buffer里能存多少条经验轨迹\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"往buffer里添加新的经验\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"从buffer里随机采样一个批次的轨迹样本\"\"\"\n",
    "        experiences = random.sample(self.buffer, k=self.batch_size)  # 随机抽取batch_size个样本\n",
    "\n",
    "        # 将变量类型从np转为tensor，并从CPU挪到GPU中进行加速计算\n",
    "        states = torch.as_tensor(np.vstack([e.state for e in experiences if e is not None]),\n",
    "                                 dtype=torch.float32, device=self.device)\n",
    "        actions = torch.as_tensor(np.vstack([e.action for e in experiences if e is not None]),\n",
    "                                  dtype=torch.float32, device=self.device)\n",
    "        rewards = torch.as_tensor(np.vstack([e.reward for e in experiences if e is not None]),\n",
    "                                  dtype=torch.float32, device=self.device)\n",
    "        next_states = torch.as_tensor(np.vstack([e.next_state for e in experiences if e is not None]),\n",
    "                                      dtype=torch.float32, device=self.device)\n",
    "        dones = torch.as_tensor(np.vstack([e.done for e in experiences if e is not None]),\n",
    "                                dtype=torch.float32, device=self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "神经网络的构建\n",
    "\"\"\"\n",
    "def init_weight(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(state_size, hidden_size), nn.ReLU(),\n",
    "                                 nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
    "                                 nn.Linear(hidden_size, hidden_size), nn.Hardswish(),\n",
    "                                 nn.Linear(hidden_size, action_size))\n",
    "        self.net.apply(init_weight)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"与环境交互并且学习好的策略\"\"\"\n",
    "    def __init__(self, state_size, action_size, hidden_size, config):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.config = config\n",
    "        self.device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Q-Network\n",
    "        self.q_net = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.q_target = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=config.lr)\n",
    "\n",
    "        # ReplayBuffer\n",
    "        self.buffer = ReplayBuffer(action_size, buffer_size=config.buffer_size, batch_size=config.batch_size)\n",
    "\n",
    "        # 训练时间步骤的初始化\n",
    "        self.t_step = 0\n",
    "        self.q_updates = 0\n",
    "        self.action_step = 4\n",
    "        self.last_action = None\n",
    "\n",
    "        self.writer = SummaryWriter('result')\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"往buffer中保存经验， 并且使用随机抽样进行学习\"\"\"\n",
    "        self.buffer.add(state, action, reward, next_state, done)  # 保存经验\n",
    "\n",
    "        # 每隔多久更新一次\n",
    "        self.t_step = self.t_step + 1\n",
    "        if (self.t_step) % self.config.update_every == 0:\n",
    "            # 如果buffe中的数据存储的够多了，就可以学习\n",
    "            if len(self.buffer) > self.config.batch_size:\n",
    "                experiences = self.buffer.sample()\n",
    "                loss = self.learn(experiences)\n",
    "                self.q_updates += 1\n",
    "                self.writer.add_scalar('q_loss', loss, self.q_updates)\n",
    "\n",
    "    def get_action(self, state, epsilon=0.):\n",
    "        # 根据当前策略返回给定状态的操作，确定性策略，画面每更新4帧多一次动作\n",
    "        state = np.array(state)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)  # 增加一个维度给batch_size\n",
    "        self.q_net.eval()\n",
    "        action_values = self.q_net(state).detach()\n",
    "        self.q_net.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            action = np.argmax(action_values.cpu().numpy())\n",
    "            self.last_action = action\n",
    "            return action\n",
    "        else:\n",
    "            action = random.choice(np.arange(self.action_size))\n",
    "            self.last_action = action\n",
    "            return action\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"\n",
    "        使用一个批次的经验轨迹数据来更新值网络和策略网络\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state)) ：这个是基于真实值的标签\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        gamma = self.config.gamma\n",
    "        alpha = self.config.alpha\n",
    "        entropy_tau = self.config.entropy_tau\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        # 从target网络模型里得到预测next的Q值(获得一堆Q值)\n",
    "        q_targets_next = self.q_target(next_states).detach()\n",
    "\n",
    "        # 用LogSumExp计算entropy，目的是维持数值的稳定性，详情见博客\n",
    "        q_k_targets_next = q_targets_next\n",
    "        v_k_targets_next = q_targets_next.max(1)[0].unsqueeze(-1)\n",
    "        logSum = torch.logsumexp((q_k_targets_next - v_k_targets_next) / entropy_tau, 1).unsqueeze(-1)\n",
    "        tau_log_pi_next = q_k_targets_next - v_k_targets_next - entropy_tau * logSum\n",
    "\n",
    "        # 目标策略\n",
    "        pi_target = F.softmax(q_targets_next / entropy_tau, dim=1)\n",
    "\n",
    "        # q_targets的计算\n",
    "        q_targets = (gamma * (pi_target * (q_targets_next - tau_log_pi_next) * (1-dones)).sum(1)).unsqueeze(-1)\n",
    "\n",
    "        # 用logSum计算munchausen的addon\n",
    "        q_k_targets = self.q_target(states).detach()\n",
    "        v_k_targets = q_k_targets.max(1)[0].unsqueeze(-1)\n",
    "        logSum = torch.logsumexp((q_k_targets - v_k_targets) / entropy_tau, 1).unsqueeze(-1)\n",
    "        tau_log_pi = q_k_targets - v_k_targets - entropy_tau * logSum\n",
    "        munchausen_addon = tau_log_pi.gather(1, actions.long())\n",
    "\n",
    "        # 计算munchausen reward\n",
    "        munchausen_reward = rewards + alpha * torch.clamp(munchausen_addon, min=-1, max=0)\n",
    "        q_targets = q_targets + munchausen_reward\n",
    "\n",
    "        # 用当前的状态去估计/预测Q值\n",
    "        q_expected = self.q_net(states).gather(1, actions.long())\n",
    "\n",
    "        # 计算loss,target是我们想去接近的（相当于真实值）\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.q_net.parameters(), max_norm=self.config.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 软更新target\n",
    "        self.soft_update(self.q_net, self.q_target, self.config.soft_update_tau)\n",
    "        return loss.detach().cpu().numpy()\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Init:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.state_size = 0\n",
    "        self.action_size = 0\n",
    "        torch.set_num_threads(self.config.num_threads)\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "\n",
    "    def init_seed(self):\n",
    "        np.random.seed(self.config.seed)\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        if torch.backends.cudnn.enabled:\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    def init_env(self):\n",
    "        env = gym.make(self.config.env_name)\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0] if isinstance(env.action_space, Box) else env.action_space.n\n",
    "        return env #, self.state_size, self.action_size\n",
    "\n",
    "    def init_agent(self):\n",
    "        agent = Agent(self.state_size, self.action_size, hidden_size=self.config.hidden_size, config=self.config)\n",
    "        return agent\n",
    "\n",
    "    def init_results_dir(self):\n",
    "        results_dir = Path('./results') / self.config.env_name / self.config.algorithm\n",
    "        # Todo: 如果存在相同名字直接覆盖\n",
    "        # if not model_dir.exists():\n",
    "        seed_dir = f'{self.config.algorithm}_{self.config.seed}'\n",
    "        logs_dir = results_dir/seed_dir\n",
    "        checkpoint_dir = logs_dir / 'checkpoint'\n",
    "        if logs_dir.exists():\n",
    "            shutil.rmtree(logs_dir, ignore_errors=True)\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        writer = SummaryWriter(logs_dir)\n",
    "        return logs_dir, checkpoint_dir, writer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from utils import epsilon_explore\n",
    "def run(config):\n",
    "    initialization = Init(config)\n",
    "    initialization.init_seed()\n",
    "    env = initialization.init_env()\n",
    "    agent = initialization.init_agent()\n",
    "    obs = env.reset()\n",
    "    score = 0\n",
    "    scores_window = deque(maxlen=100)\n",
    "    frames = config.frames\n",
    "    for frame in range(1, frames):\n",
    "        epsilon = epsilon_explore(frame, frames)\n",
    "        action = agent.get_action(obs, epsilon)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        agent.step(obs, action, reward, next_obs, done)\n",
    "        obs = next_obs\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            scores_window.append(score)\n",
    "            agent.writer.add_scalar('Average 100', np.mean(scores_window), frame)\n",
    "            obs = env.reset()\n",
    "            score = 0\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--algorithm ALGORITHM] [--run_num RUN_NUM]\n",
      "                             [--n_episodes N_EPISODES]\n",
      "                             [--num_threads NUM_THREADS] [--gamma GAMMA]\n",
      "                             [--frames FRAMES] [--eps_frames EPS_FRAMES]\n",
      "                             [--min_eps MIN_EPS]\n",
      "                             [--experiment_name EXPERIMENT_NAME] [--seed SEED]\n",
      "                             [--cuda] [--cuda_deterministic]\n",
      "                             [--env_name ENV_NAME] [--action_step ACTION_STEP]\n",
      "                             [--buffer_size BUFFER_SIZE]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--episode_length EPISODE_LENGTH]\n",
      "                             [--n_step N_STEP]\n",
      "                             [--soft_update_tau SOFT_UPDATE_TAU]\n",
      "                             [--hidden_size HIDDEN_SIZE] [--layer_N LAYER_N]\n",
      "                             [--lr LR] [--critic_lr CRITIC_LR]\n",
      "                             [--opti_eps OPTI_EPS]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--update_every UPDATE_EVERY]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--entropy_tau ENTROPY_TAU] [--alpha ALPHA]\n",
      "                             [--use_linear_lr_decay]\n",
      "                             [--save_interval SAVE_INTERVAL]\n",
      "                             [--log_interval LOG_INTERVAL]\n",
      "                             [--model_dir MODEL_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/meta/.local/share/jupyter/runtime/kernel-38105613-97ae-4a54-be7d-dcf8e0927d7a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = get_config()\n",
    "    config = parser.parse_args()\n",
    "    run(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}